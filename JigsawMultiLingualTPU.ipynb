{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5116  100  5116    0     0   3311      0  0:00:01  0:00:01 --:--:--  3309\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-dev20200515 ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+35d732a\nUninstalling torchvision-0.6.0a0+35d732a:\nDone updating TPU runtime\n  Successfully uninstalled torchvision-0.6.0a0+35d732a\nCopying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \nOperation completed over 1 objects/91.0 MiB.                                     \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n\\ [1 files][119.5 MiB/119.5 MiB]                                                \nOperation completed over 1 objects/119.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  2.3 MiB/  2.3 MiB]                                                \nOperation completed over 1 objects/2.3 MiB.                                      \nProcessing ./torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (1.18.5)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.6.0a0+bf2bbd9\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+2b2085a\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (7.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.18.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly+20200515) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.7.0a0+a6073f0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 0s (21.0 MB/s) \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 107745 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch\nfrom tqdm import tqdm\nimport transformers\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/bertbasemultilingualuncased/bert-base-multilingual-uncased/vocab.txt\n/kaggle/input/bertbasemultilingualuncased/bert-base-multilingual-uncased/config.json\n/kaggle/input/bertbasemultilingualuncased/bert-base-multilingual-uncased/pytorch_model.bin\n","name":"stdout"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 128\nVALID_BATCH_SIZE = 64\nEPOCHS = 10\nACCUMULATION = 2\nBERT_PATH = \"../input/bertbasemultilingualuncased/bert-base-multilingual-uncased\"\nMODEL_PATH = \"model.bin\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, comment_text, target):\n        self.comment_text = comment_text\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    \n    def __len__(self):\n        return len(self.comment_text)\n    \n\n    def __getitem__(self, item):\n        comment_text = self.comment_text[item]\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len\n        )\n\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n\n        padding_len = self.max_len  - len(ids)\n        ids = ids + ([0] * padding_len)\n        mask = mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n\n        return {\n            \"ids\" : torch.tensor(ids, dtype = torch.long),\n            \"mask\" : torch.tensor(mask, dtype = torch.long),\n            \"token_type_ids\" : torch.tensor(token_type_ids, dtype = torch.long),\n            \"target\" : torch.tensor(self.target[item], dtype = torch.float)\n        }\n","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768 * 2, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        o1, _ = self.bert(\n            ids,\n            attention_mask = mask,\n            token_type_ids = token_type_ids\n        )\n\n        mean_pooling = torch.mean(o1, 1)\n        max_pooling, _ = torch.max(o1, 1)\n        cat = torch.cat((mean_pooling, max_pooling), 1)\n\n        bo = self.bert_drop(cat)\n        output = self.out(bo)\n        return output\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, target):\n    return nn.BCEWithLogitsLoss()(outputs, target.view(-1, 1))\n\n\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    for bi, d in enumerate(data_loader):\n        ids = d['ids']\n        mask = d['mask']\n        token_type_ids = d['token_type_ids']\n        target = d['target']\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype = torch.long)\n        token_type_ids = token_type_ids.to(device, dtype = torch.long)\n        target = target.to(device, dtype = torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids = ids,\n            mask = mask,\n            token_type_ids = token_type_ids\n        )\n\n        loss = loss_fn(outputs, target)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        if bi % 10 == 0:\n            xm.master_print(\"bi = {}, loss = {}\".format(bi, loss))\n        \n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in enumerate(data_loader):\n            ids = d['ids']\n            mask = d['mask']\n            token_type_ids = d['token_type_ids']\n            target = d['target']\n\n            ids = ids.to(device, dtype = torch.long)\n            mask = mask.to(device, dtype = torch.long)\n            token_type_ids = token_type_ids.to(device, dtype = torch.long)\n            target = target.to(device, dtype = torch.float)\n\n            outputs = model(\n                ids = ids,\n                mask = mask,\n                token_type_ids = token_type_ids\n            )\n            fin_targets.extend(target.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n    df1 = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv', usecols=['comment_text', 'toxic'])\n    df2 = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv', usecols=['comment_text', 'toxic'])\n\n    df_train = pd.concat([df1, df2], axis=0)\n    df_train = df_train.reset_index(drop=True)\n\n    df_valid = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\n\n    train_dataset = BERTDataset(\n        comment_text = df_train.comment_text.values,\n        target = df_train.toxic.values\n    )\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset,\n    num_replicas=xm.xrt_world_size(), rank = xm.get_ordinal(), shuffle=True\n    )\n    \n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=1,\n        sampler = train_sampler,\n        drop_last=True\n    )\n\n    valid_dataset = BERTDataset(\n        comment_text = df_valid.comment_text.values,\n        target = df_valid.toxic.values\n    )\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n    valid_dataset,\n    num_replicas=xm.xrt_world_size(),\n    rank = xm.get_ordinal(),\n    shuffle=True\n    )\n    \n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1,\n        sampler = valid_sampler\n    )\n\n    device = xm.xla_device()\n    model = BERTBaseUncased()\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n    lr = 3e-5 * xm.xrt_world_size()\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        print(\"here\")\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_fn(para_loader.per_device_loader(device), model, optimizer,device, scheduler)\n        valid_para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        outputs, targets = eval_fn(valid_para_loader, model, device)\n        targets = np.array(targets) >=0.5\n        accuracy = metrics.roc_auc_score(targets, outputs)\n        print(\"accuracy_score = {accuracy}\".format(accuracy=accuracy))\n        if(accuracy>best_accuracy):\n            xm.save(model.state_dict(), MODEL_PATH)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla.distributed.xla_multiprocessing as xmp","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp(rank, flags):\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\n    a = run()\n\n\nxmp.spawn(_mp, args=({}, ), nprocs=1, start_method='fork')","execution_count":29,"outputs":[{"output_type":"stream","text":"here\nbi = 0, loss = 0.6949639916419983\nbi = 10, loss = 0.5543463230133057\nbi = 20, loss = 0.2979893088340759\nbi = 30, loss = 0.3130144476890564\nbi = 40, loss = 0.2549452781677246\nbi = 50, loss = 0.29970675706863403\nbi = 60, loss = 0.244351327419281\nbi = 70, loss = 0.38717958331108093\nbi = 80, loss = 0.356704980134964\nbi = 90, loss = 0.32659977674484253\nbi = 100, loss = 0.34495478868484497\nbi = 110, loss = 0.2458876669406891\nbi = 120, loss = 0.2737136781215668\nbi = 130, loss = 0.21093273162841797\nbi = 140, loss = 0.2990308701992035\nbi = 150, loss = 0.2106209546327591\nbi = 160, loss = 0.2089516520500183\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-3146927de1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0mpf_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_fork_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_devices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0m_start_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     return torch.multiprocessing.start_processes(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36m_start_fn\u001b[0;34m(index, pf_cfg, fn, args)\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     print(\n","\u001b[0;32m<ipython-input-29-3146927de1e9>\u001b[0m in \u001b[0;36m_mp\u001b[0;34m(rank, flags)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch.FloatTensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-ce376cc81f61>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpara_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mvalid_para_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParallelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_para_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-ca889f461d5f>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    121\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    122\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}